<HEAD>
<meta name="keywords" content="Anqi Mao, Mehryar Mohri, Mohri, Corinna Cortes, Pranjal Awasthi, machine learning, learning theory, applied mathematics, mathematics, computer science, CIMS, Courant, Google Research, Google">
<TITLE>Anqi Mao</TITLE>
</HEAD>

<BODY BGCOLOR="WHITE">


<td valign=top>
<font size="3">

<img src="ID_photo.jpg" width="100" height="auto" style="float: left; margin-right: 10px;"> 

<a name=n><h1 style="font-weight: lighter">Anqi Mao <span style="font-size:14.0pt">(</span><a href="https://scholar.google.com/citations?user=nkjIZ-oAAAAJ&hl=en&oi=ao"><span style="font-size:14.0pt">Google Scholar</span></a><span style="font-size:14.0pt">)</span>
</h1>
</div>
  
<p>I am a Ph.D. Candidate in Mathematics at the Courant Institute of Mathematical Sciences, New York University, where I am very fortunate to be advised by <a href="https://cs.nyu.edu/~mohri/">Prof. Mehryar Mohri</a>. My main research interests are machine learning theory and algorithms.
<br>


  <a name=pubs><h4>Publications </h4>
  <ol>

<li style="margin-bottom: 10px;">
    Corinna Cortes, Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://arxiv.org/pdf/2407.07140">
  Cardinality-Aware Set Prediction and Top-k Classification.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2024)</em>. Vancouver, Canada, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://arxiv.org/pdf/2407.13732">
  Realizable H-Consistent and Bayes-Consistent Loss Functions for Learning to Defer.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2024)</em>. Vancouver, Canada, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://arxiv.org/pdf/2405.05968">
  A Universal Growth Rate for Learning with Smooth Surrogate Losses.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2024)</em>. Vancouver, Canada, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://arxiv.org/pdf/2407.13746">
  Multi-Label Learning with Stronger Consistency Guarantees.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2024)</em>. Vancouver, Canada, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
<a href="https://proceedings.mlr.press/v235/mao24d.html">
  Regression with Multi-Expert Deferral.</a><br>
  In <em>Proceedings of the 41st International Conference on Machine Learning (ICML 2024)</em>. Vienna, Austria, 2024.
 <br><b>(Spotlight Presentation)</b>
  </li>

  <li style="margin-bottom: 10px;">
    Raef Bassily, Corinna Cortes, Anqi Mao, and Mehryar Mohri. <br>
<a href="https://proceedings.mlr.press/v235/bassily24a.html">
  Differentially Private Domain Adaptation with Theoretical Guarantees.</a><br>
  In <em>Proceedings of the 41st International Conference on Machine Learning (ICML 2024)</em>. Vienna, Austria, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
<a href="https://proceedings.mlr.press/v235/mao24c.html">
  H-Consistency Guarantees for Regression.</a><br>
  In <em>Proceedings of the 41st International Conference on Machine Learning (ICML 2024)</em>. Vienna, Austria, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
<a href="https://cs.nyu.edu/~mohri/pub/mabsc.pdf">
  Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention.</a><br>
  In <em>Twenty-seventh Conference on Artificial Intelligence and Statistics (AISTATS 2024)</em>. Valencia, Spain, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and Yutao Zhong. <br>
  <a href="https://openreview.net/pdf?id=dCHbFDsCZz">
  Learning to Reject with a Fixed Predictor: Application to Decontextualization.</a><br>
  In <em>Twelfth International Conference on Learning Representations (ICLR 2024)</em>. Vienna, Austria, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/mabs.pdf">
  Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms.</a><br>
  In <em>Proceedings of the 35th International Conference on Algorithmic Learning Theory (ALT 2024)</em>. San Diego, California, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/def.pdf">
  Principled Approaches for Learning to Defer with Multiple Experts.</a><br>
  In <em>International Symposium on Artificial Intelligence and Mathematics (ISAIM 2024)</em>. Fort Lauderdale, Florida, 2024.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/tdef.pdf">
  Two-stage learning to defer with multiple experts.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2023)</em>. New Orleans, Louisiana, 2023.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/sphcb.pdf">
  Structured prediction with stronger consistency guarantees.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2023)</em>. New Orleans, Louisiana, 2023.
  </li>
  
  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/chcb.pdf">  
  H-consistency bounds: characterization and extensions.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2023)</em>. New Orleans, Louisiana, 2023.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/comp.pdf">
  Cross-entropy loss functions: Theoretical analysis and applications.</a><br>
  In <em>Proceedings of the 40th International Conference on Machine Learning (ICML 2023)</em>. Honolulu, Hawaii, 2023.
  </li>

  <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/hcbr.pdf">
  H-consistency bounds for pairwise misranking loss surrogates.
  </a><br>
  In <em>Proceedings of the 40th International Conference on Machine Learning (ICML 2023)</em>. Honolulu, Hawaii, 2023.
  </li>
  
    <li style="margin-bottom: 10px;">
    Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://arxiv.org/pdf/2307.02035.pdf">
  Ranking with Abstention.
  </a><br>
  <em>ICML 2023 Workshop on the Many Facets of Preference-Based Learning</em>. Honolulu, Hawaii, 2023.
  </li>

  <li style="margin-bottom: 10px;">
    Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/dcnn.pdf">
  DC-programming for neural network optimizations.</a><br>
  <em>Journal of Global Optimization (JOGO)</em>, 2023.
  </li>

  <li style="margin-bottom: 10px;">
    Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/aar.pdf">
  Theoretically grounded loss functions and algorithms for adversarial robustness.</a><br>
  In <em>Twenty-sixth Conference on Artificial Intelligence and Statistics (AISTATS 2023)</em>. Valencia, Spain, 2023.
  </li>


  <li style="margin-bottom: 10px;">
    Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/mhcb.pdf">
  Multi-class H-consistency bounds.</a><br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2022)</em>. New Orleans, Louisiana, 2022.
  </li>


  <li style="margin-bottom: 10px;">
  Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/hcb.pdf">
  H-consistency bounds for surrogate loss minimizers.
  </a>
  <br>
  In <em>Proceedings of the 39th International Conference on Machine Learning (ICML 2022)</em>. Baltimore, MD, 2022.
  <br><b>(Long Presentation)</b>
  </li>

  <li style="margin-bottom: 10px;">
    Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://arxiv.org/pdf/2105.01550.pdf">
  A Finer Calibration Analysis for Adversarial Robustness.</a><br>
  <em>CoRR</em>, abs/2105.01550, 2021.
  </li>

  <li style="margin-bottom: 10px;">
  Pranjal Awasthi, Natalie S. Frank, Anqi Mao, Mehryar Mohri, and Yutao Zhong. <br>
  <a href="https://cs.nyu.edu/~mohri/pub/advcons.pdf">
  Calibration and consistency of adversarial surrogate losses.
  </a>
  <br>
  In <em>Advances in Neural Information Processing Systems (NeurIPS 2021)</em>. Online, 2021.
  <br><b>(Spotlight Presentation)</b>
  </li>
  
  <li style="margin-bottom: 10px;">
  Yingzhou Li, Jianfeng Lu, and Anqi Mao. <br>
  <a href="https://yingzhouli.com/download_file/Paper/solvetrain.pdf">
  Variational training of neural network approximations of solution maps for physical models.
  </a>
  <br>
  <em>Journal of Computational Physics</em>, 2020.
  </li>


  </ol>
  
  <a name=pubs><h4>Service</h4>

  <ul>

  <li>
  Reviewer:
  
  <ul>
  <li>
  ICML: 2022 (top 10%), 2023, 2024
  </li>
  <li>
  NeurIPS: 2022, 2023, 2024
  </li>
  <li>
  AISTATS: 2023
  </li>
  <li>
  COLT: 2022
  </li>
  <li>
  Annals of Mathematics and Artificial Intelligence
  </li>
  </ul>
  
  <li>
  Session Manager:
  </li>
  
  <ul>
  <li>
  ICML: 2022
  </li>
  </ul>
  
  </ul>
  </li>
  
  </ul>
  

  <a name=pubs><h4>Teaching</h4>

  <ul>

  <li>
  <a href="https://cs.nyu.edu/~mohri/aml24/">
  Advanced Machine Learning</a> (Teaching Assistant and Grader, NYU Spring 2024)
  </li>

  <li>
  <a href="https://cs.nyu.edu/~mohri/ml24/">
  Foundations of Machine Learning</a> (Teaching Assistant and Grader, NYU Fall 2024)
  </li>

  <li>
  <a href="https://cs.nyu.edu/~mohri/ml23/">
  Foundations of Machine Learning</a> (Teaching Assistant and Grader, NYU Fall 2023)
  </li>

  <li>
  <a href="https://cs.nyu.edu/~mohri/mlsp23/">
  Foundations of Machine Learning</a> (Teaching Assistant and Grader, NYU Spring 2023)
  </li>
  
  <li>
  <a href="https://cs.nyu.edu/~mohri/mlsp22/">
  Foundations of Machine Learning</a> (Teaching Assistant and Grader, NYU Spring 2022)
  </li>

  </ul>
  
  <a name=pubs><h4>Contact</h4>

  <ul>

  <li>
  E-mail: aqmao[at]cims[dot]nyu[dot]edu
  </li>

  </ul>

  </font>

